{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4956935e",
   "metadata": {},
   "source": [
    "# Uncertainty Prediction\n",
    "\n",
    "We often consider model confidence to avoid making costly mistakes when making decisions based on model predictions. Two types of uncertainty can be introduced to our models. Aleatoric uncertainty is irreducible and stems from the random data generation process (for example, inaccurate measurements). The lack of relevant data causes epistemic uncertainty and can be fixed by generating more data points.\n",
    "\n",
    "![uncertainty](uncertainty.png)\n",
    "\n",
    "<span style=\"font-size: 8pt\">source: https://towardsdatascience.com/my-deep-learning-model-says-sorry-i-dont-know-the-answer-that-s-absolutely-ok-50ffa562cb0b</span>\n",
    "\n",
    "## Conformal Prediction\n",
    "\n",
    "One uncertainty prediction method that is agnostic to the selected machine learning model is conformal prediction. It's a method based on statistical testing that instead of returning point-predictions (one value), it predicts set-predictions (range of possible values under uncertainty).\n",
    "\n",
    "This model is based on so-called non-conformity scores defined by the function $r:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$. In the case of classification, this measures how much data examples do not conform to the predicted class. Let's say $r(x, y) = 1 - f_y(x)$ where $f_y:\\mathcal{X}\\to[0,1]$ is the model prediction for the given class $y$ (i.e. the probability of **not** predicting this class).\n",
    "\n",
    "The first step is to use a calibration set to compute non-conformity scores. Let's assume that our calibration set is a subset of the dataset defined by indices $I$.\n",
    "\n",
    "Next, we can predict p-values for class $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "p_y(x) = \\frac{|\\{i\\in I\\,|\\, r(x_i, y_i) \\geq r(x,y)\\}|}{| \\{ i\\in I \\} |}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the conformal prediction at the confidence level $\\alpha$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{conf}^\\alpha(x) = \\{ y\\in\\mathcal{Y} \\,|\\, p_y(x) \\geq \\alpha \\}\n",
    "\\end{equation}\n",
    "\n",
    "**Theorem:** Let $\\alpha\\in[0,1]$ and let $(x_i,y_i)$, $i=1,\\dots,n$ i.i.d. Then for a new i.i.d. pair $(x_{n+1},y_{n+1})$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\left(y_{n+1}\\in C_{conf}^\\alpha(x_{n+1})\\right) \\geq 1 - \\alpha.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550348b",
   "metadata": {},
   "source": [
    "**Exercise 1:** Implement conformal prediction according to the formulas above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43b7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X_test, y_test):\n",
    "    return model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "data = ADME('CYP3A4_Veith')\n",
    "split = data.get_split(method='scaffold')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='Y')\n",
    "scores = []\n",
    "\n",
    "roc_auc = Evaluator(name = 'ROC AUC')\n",
    "accuracy = Evaluator(name = 'Accuracy')\n",
    "\n",
    "X_train, y_train = featurizer(split['train'])\n",
    "X_calibration, y_calibration = featurizer(split['valid'])\n",
    "X_test, y_test = featurizer(split['test'])\n",
    "model = train(X_train, y_train)\n",
    "predictions = predict(model, X_test, y_test)\n",
    "roc_auc_score = roc_auc(y_test, predictions)\n",
    "accuracy_score = accuracy(y_test, predictions)\n",
    "print(f'ROC AUC, accuracy = {roc_auc_score}, {accuracy_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da6acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model: sklearn.base.BaseEstimator, X_calibration: np.ndarray, y_calibration: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calibrates model prediction on a calibration set.\n",
    "    \n",
    "    Args:\n",
    "        model: sklearn model to calibrate\n",
    "        X_calibration: fingerprints/descriptors of the calibration compounds\n",
    "        y_calibration: labels of the calibration compounds (can be used for conditional calibration)\n",
    "    \n",
    "    Returns:\n",
    "        Array of non-conformity scores for one of the classes (for example negative class).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "def compute_pvalue(model: sklearn.base.BaseEstimator, X: np.ndarray, y: int, calibration_scores: np.ndarray):\n",
    "    \"\"\"Computes p-values for the given prediction class y.\n",
    "    \n",
    "    Args:\n",
    "        model: sklearn model used for prediction\n",
    "        X: fingerprints/descriptors of the input compounds\n",
    "        y: prediction class\n",
    "        calibration_scores: set of non-conformity scores calculated on the calibration set\n",
    "    \n",
    "    Returns:\n",
    "        Array of p-values for all model predictions, given the prediction class y.\n",
    "    \"\"\"\n",
    "    preds = model.predict_proba(X)[:, 1]\n",
    "    if y == 1:\n",
    "        # TODO: calculate non-conformity scores for the predictions and adjust the calibration scores\n",
    "    else:\n",
    "        # TODO: do the same thing for the negative class\n",
    "    pvalues = []\n",
    "    for r in nonconformity_scores:\n",
    "        pvalue = ...  # TODO: compute p-value\n",
    "        pvalues.append(pvalue)\n",
    "    return np.array(pvalues)\n",
    "\n",
    "def predict_conformal(\n",
    "    model: sklearn.base.BaseEstimator,\n",
    "    X: np.ndarray,\n",
    "    calibration_scores: np.ndarray,\n",
    "    alpha: float,\n",
    "    return_proba: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calculate conformal predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: sklearn model used for prediction\n",
    "        X: fingerprints/descriptors of the input compounds\n",
    "        calibration_scores: set of non-conformity scores calculated on the calibration set\n",
    "        alpha: confidence level\n",
    "        return_proba: if True, probabilities are returned as predictions (not conformal prediction classes)\n",
    "    \n",
    "    Returns:\n",
    "        Conformal predictions and masks corresponding to the certain predictions at the confidence level alpha.\n",
    "    \"\"\"\n",
    "    pvalues0 = ...  # TODO: compute p-values for class 0\n",
    "    pvalues1 = ...  # TODO: compute p-values for class 1\n",
    "    mask = ...  # TODO: compute prediction masks (only certain predictions)\n",
    "    conformal_preds = ...  # TODO: return predictions that can be either probabilities returned by the model \n",
    "                           # or classes corresponding to the conformal prediction set\n",
    "    return conformal_preds, mask\n",
    "\n",
    "alpha = 0.95\n",
    "calibration_scores = calibrate(model, X_calibration, y_calibration)\n",
    "conformal_preds, mask = predict_conformal(model, X_test, calibration_scores, alpha)\n",
    "\n",
    "roc_auc_score = roc_auc(y_test[mask], conformal_preds[mask])\n",
    "accuracy_score = accuracy(y_test[mask], conformal_preds[mask])\n",
    "print(f'ROC AUC, accuracy, n = {roc_auc_score}, {accuracy_score}, {sum(pvalues >= alpha)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcab90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "alpha = np.linspace(0, 1)\n",
    "\n",
    "preds, labels = [], []\n",
    "for a in tqdm(alpha):\n",
    "    conformal_preds, mask = predict_conformal(model, X_test, calibration_scores, a)\n",
    "    y_test_masked = y_test[mask]\n",
    "    preds_masked = conformal_preds[mask]\n",
    "    preds.append(preds_masked)\n",
    "    labels.append(y_test_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d7304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(alpha, [roc_auc(l, p) if len(l) else 1. for l, p in zip(labels, preds)])\n",
    "axes[0].set_xlabel('confidence')\n",
    "axes[0].set_ylabel('ROC AUC')\n",
    "\n",
    "axes[1].plot(alpha, [accuracy(l, p) for l, p in zip(labels, preds)])\n",
    "axes[1].set_xlabel('confidence')\n",
    "axes[1].set_ylabel('accuracy')\n",
    "\n",
    "axes[2].plot(alpha, [len(l) for l in labels])\n",
    "axes[2].set_xlabel('confidence')\n",
    "axes[2].set_ylabel('efficiency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e07f2",
   "metadata": {},
   "source": [
    "## Uncertainty Prediction in Deep Learning\n",
    "\n",
    "[1] Abdar, Moloud, et al. \"A review of uncertainty quantification in deep learning: Techniques, applications and challenges.\" *Information Fusion* 76 (2021): 243-297.\n",
    "\n",
    "![DL](dl_techniques.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
