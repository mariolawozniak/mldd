{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3bc77c",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Three components are necessary to train a neural network:\n",
    "1. neural network architecture\n",
    "2. loss function\n",
    "3. optimization algorithm\n",
    "\n",
    "All these components can be easily implemented using one of the deep learning libraries for Python, e.g. PyTorch, TensorFlow, or JAX. You can find also some higher-level libraries built on top of these low-level libraries, e.g. Kera or Sonnet for TensorFlow or Haiku for JAX. We will be using PyTorch and PyTorch-Geometric to build graph neural networks.\n",
    "\n",
    "\n",
    "## Neural network architecture: model capacity and inductive bias\n",
    "\n",
    "Neural networks usually consist of layers, i.e. a network is a sequence of neural layers that transform the input representation. The most basic layer in neural networks is probably a **linear** or **fully-connected layer**. All input features are connected to all layer output features (neurons), and the strength of these connections is learned when the network is trained. Mathematically, we can write:\n",
    "\n",
    "$$\n",
    "[x_1, \\dots, x_n] \\cdot \\begin{bmatrix} \n",
    "w_{11} & \\cdots & w_{1m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n1} & \\cdots & w_{nm}\n",
    "\\end{bmatrix} = \\mathbf{x}^T W = \\mathbf{y}^T =[y_1, \\dots, y_m],\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^n$ is the layer input, and $\\mathbf{y}\\in\\mathbb{R}^m$ is the layer output. \n",
    "$W\\in\\mathbb{R}^{n\\times m}$ are layer parameters called **weights**. Usually a **bias** term is added to the transformed output, i.e. $\\mathbf{x}^T W + \\mathbf{b}^T$. Let's note that single output neurons follow the equation similar to linear regression:\n",
    "\n",
    "$$\n",
    "y_k = x_1 w_{1k} + x_2 w_{2k} + \\cdots + x_n w_{nk} + b_k\n",
    "$$\n",
    "\n",
    "Another important class of layers are **non-linear layers** (or **activation layers**). They usually don't contain trainable parameters and are used only to introduce non-linearity to the model. Without them, only linear depenencies could be learned, so the network wouldn't be able to learn anything more than a simple linear regression. Currently, the most widely used activation is **ReLU** (Rectified Linear Unit) because of its effectiveness in terms of the computational cost and training outcomes. The ReLU layer is defined as:\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\max(x,0) = \\begin{cases} x & \\text{dla } x \\geq 0,\\\\ 0 & \\text{dla } x < 0. \\end{cases}\n",
    "$$\n",
    "\n",
    "Another important type of non-linearity is **sigmoid**, which is less effective computation-wise. Sigmoid was used also in the logistic regression for binary classification. It returns numbers in the range (0, 1), which can be treated as positive class probabilities.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "## Loss function: problem definition and optimization target\n",
    "\n",
    "The neural architecture defines the transformations that the network is capable of performing, but it doesn't define the training task itself. To determine what our network should learn, we will need a **loss function**, which is an optimization target. It's a function that will me minimized (rarely maximized) by modifying network parameters (weights).\n",
    "\n",
    "For example, for the regression problem, a standard loss function is the mean squared error (MSE) that measures the average error of all predictions, squared:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{X}|\\Theta)=\\mathrm{MSE}(\\mathcal{X}|\\Theta) = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} e_i^2 = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} (y_i - \\hat{y}_i)^2 = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} (y_i - f(\\mathbf{x}_i|\\Theta))^2\n",
    "$$ \n",
    "\n",
    "We'll be minimizing this function with respect to the parameters $\\Theta$, which are network weights.\n",
    "\n",
    "$$\n",
    "\\Theta^* = {\\arg \\min}_\\Theta \\,\\, \\mathcal{L}(\\mathcal{X}|\\Theta)\n",
    "$$\n",
    "\n",
    "## Optimization algorithm: method to reach the minimum of the loss function\n",
    "\n",
    "Having defined the architecture and loss function, we need to choose an algorithm that will find optimal model parameters. In the classical machine learning, we oftentimes analytically derived the formulas to find optimal parameters. The diversity and complexity of neural networks make it infeasible to find such solutions for these models.\n",
    "\n",
    "However, we can use **gradient optimization** methods, which compute the loss function gradients (derivatives) w.r.t. the model parameters and move these parameters towards the direction opposite to the gradient (if our goal is to minimize the loss function). It is possible because of the differentiability of all operations in a neural network and the **chain rule**.\n",
    "\n",
    "The most standard algorithm is **SGD** (Stochastic Gradient Descent):\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\nabla \\mathcal{L}(\\mathcal{X}|\\Theta),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is learning rate, which can be selected for a specific task (it's a hyperparameter). The SGD method is the most basic, and many follow-up methods have been implemented in deep learning libraries. For example, **Momentum** is a technique that accumulates past gradients to accelerate the convergence and to avoid local minima.\n",
    "\n",
    "$$\n",
    "v \\leftarrow \\gamma v + \\eta \\nabla \\mathcal{L}(\\mathcal{X}|\\Theta), \\\\\n",
    "\\Theta \\leftarrow \\Theta - v.\n",
    "$$\n",
    "\n",
    "The momentum constant $\\gamma$ is usually set to 0.9 (it should be less than 1 to gradually decrease the momentum).\n",
    "\n",
    "**Exercise 1:** Fill in the neural network training code by implementing the three training components mentioned above. This code uses Morgan fingerprints (ECFP) to predict the compound solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce1528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "data = ADME('Solubility_AqSolDB')\n",
    "split = data.get_split()\n",
    "rmse = Evaluator(name = 'RMSE')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='Y')\n",
    "X_train, y_train = featurizer(split['train'])\n",
    "X_valid, y_valid = featurizer(split['valid'])\n",
    "X_test, y_test = featurizer(split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada1c4c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_valid, y_valid):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # model preparation\n",
    "    model = torch.nn.Sequential(\n",
    "      torch.nn.Linear(1024, 800),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(800, 400),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Linear(400, 2),\n",
    "      torch.nn.Linear(2, 1)\n",
    "    )  # TODO: define a simple neural network, you can use torch.nn.Sequential\n",
    "    \n",
    "    # data preparation\n",
    "    dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.reshape(-1, 1)))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid.reshape(-1, 1)))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # prepare metrics plots\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(7, 3), layout=\"constrained\")\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    df_metrics = pd.DataFrame()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # TODO: define an optimizer\n",
    "    loss_fn = torch.nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        model.train()\n",
    "        for X, y in tqdm(loader, leave=False):\n",
    "            model.zero_grad()\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # validation loop\n",
    "        model.eval()\n",
    "        preds_batches = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_loader, leave=False):\n",
    "                preds = model(X)\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        rmse_score = rmse(y_valid, preds)\n",
    "        \n",
    "        df_metrics = df_metrics.append({'epoch': epoch, 'loss': loss.item(), 'rmse': rmse_score}, ignore_index=True)\n",
    "        ax[0].clear()\n",
    "        ax[0].plot(df_metrics.epoch, df_metrics.loss)\n",
    "        ax[0].set_title('training loss')\n",
    "        ax[0].set_xlabel('epoch')\n",
    "        ax[0].set_ylabel('MSE')\n",
    "        ax[1].clear()\n",
    "        ax[1].plot(df_metrics.epoch, df_metrics.rmse)\n",
    "        ax[1].set_title('validation RMSE')\n",
    "        ax[1].set_xlabel('epoch')\n",
    "        ax[1].set_ylabel('RMSE')\n",
    "        dh.update(fig)\n",
    "    plt.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X_test, y_test):\n",
    "    # hyperparameters definition\n",
    "    # (but this doesn't change the training results, it's only to optimize the eval speed)\n",
    "    batch_size = 64\n",
    "\n",
    "    # data preparation\n",
    "    dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.reshape(-1, 1)))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(loader):\n",
    "            preds = model(X)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds\n",
    "\n",
    "# training\n",
    "model = train(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, X_test, y_test)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions)\n",
    "print(f'RMSE = {rmse_score:.3f}')\n",
    "assert rmse_score < 1.6, \"It should be possible to obtain RMSE lower than 1.6\"\n",
    "print('Looks OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a217d",
   "metadata": {},
   "source": [
    "# Molecular graphs\n",
    "\n",
    "**Recap:** In mathematics, a graph is an object that consists of a set of vertices (nodes) connected with edges, i.e. $\\mathcal{G} = (V, E)$, where $V = \\{ v_i: i \\in \\{1, 2, \\dots, N \\} \\}$ and $E \\subseteq \\{ (v_i, v_j):\\, v_i,v_j \\in V \\}$.\n",
    "\n",
    "Molecular graphs are a special class of graphs, where besides nodes (denoting atoms) and edges (denoting chemical bonds), we have an additional information about atom types and sometimes also bond types. We can assume that we have an additional set of node/atom features encoded as a matrix $X$, where $X_{ij}$ is the $j$-th feature of the $i$-th atom. As atomic features, we can have one-hot encoded atom symbols (a vector containing zeros on all positions besides the position that corresponds to the atom symbol), the number of implicit hydrogens bonded with this atom, or the number of heavy neighbors (atoms other than hydrogens bonded to the given atom).\n",
    "\n",
    "Egdes/bonds can be encoded in two different ways. One method is to use an adjacency matrix $A$, where $A_{ij}=1$ if nodes/atoms $v_i$ nad $v_j$ are connected ($A_{ij}=0$ otherwise). In the case of sparse matrices, a more useful encoding is a list of pairs of connected atoms (a list of index pairs). This latter enocding is used by the PyTorch-Geometric library.\n",
    "\n",
    "In practice, a molecular graph can be described by two matrices: $X \\in \\mathbb{R}^{N \\times F}$ and $E \\in \\{0, 1,\\dots,N-1\\}^{2 \\times N}$, where $N$ is the number of atoms, and $F$ is the number of atomic features.\n",
    "\n",
    "![molecular graph](../../lectures/assets/mol_graph.png)\n",
    "\n",
    "**Exercise 2:** Create a molecular graph dataset using PyTorch-Geometric and the same data as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ed7f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise ValueError(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "class GraphFeaturizer(Featurizer):\n",
    "    def __call__(self, df):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                ...  # TODO: Add edges in both directions\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                results = ...  # TODO: Add atom features as a list, you can use one_of_k_encodings defined above\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609d4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = GraphFeaturizer('Y')\n",
    "graph = featurizer(split['test'].iloc[:1])[0]\n",
    "\n",
    "assert graph.x.ndim == 2, 'The atom features should be a matrix with dimensions (number of atoms) x (number of features)'\n",
    "assert graph.edge_index.ndim == 2, 'The edges should be represented as a matrix of atom pairs'\n",
    "assert graph.edge_index.shape[0] == 2, 'The first dimension should be 2 (we need atom pairs)'\n",
    "assert isinstance(graph.y, torch.FloatTensor) and graph.y.shape == (1,), 'The graph label should be assigned to the variable `y`'\n",
    "print('Looks OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051f97",
   "metadata": {},
   "source": [
    "# Graph neural networks\n",
    "\n",
    "Graph neural networks (GNNs) can process graph representation given at the input to the model. A graph layer uses the input atom features and graph topology to calculate new atom representations, but the graph structure is not changed (the edges stay the same). We can use the calculated atom representations for **node classification**, or we can add a graph readout operation at the end to aggregate information from all nodes into one vector that describes the whole graph (e.g. we can use the average atom representation). The graph representation can be then processed by fully-connected layers for the **graph classification** task.\n",
    "\n",
    "\n",
    "## Message Passing Neural Networks (MPNN)\n",
    "\n",
    "MPNN is a general description of a graph neural network, and many graph neural network architectures can be matched with this description (including the ones listed below). In MPNN, **messages** $M$ from the neighboring atoms are retrieved to calculate a new atom representation via the **update** $U$ operation. This procedure is repeated a few times before we use **readout** $R$ to compute the graph representation.\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_i^{t+1} = \\sum_{j\\in\\mathcal{N}(i)} M_t(\\mathbf{h}_i^t, \\mathbf{h}_j^t, \\mathbf{e}_{ij})\\\\\n",
    "\\mathbf{h}_i^{t+1} = U_t(\\mathbf{h}_i^t, \\mathbf{m}_i^{t+1}) \\\\\n",
    "\\hat{y} = R(\\{\\mathbf{h}_i^T \\, | \\, i \\in \\mathcal{G} \\})\n",
    "$$\n",
    "\n",
    "- **Graph Convolutional Networks (GCN)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W^T \\sum_{j\\in\\mathcal{N}(i)\\cup \\{i\\}} \\frac{e_{ij}}{\\sqrt{\\hat{d}_i \\hat{d}_j}} \\mathbf{h}_j^t\n",
    "$$\n",
    "\n",
    "- **Graph Isomorphism Networks (GIN)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W^T \\left( (1+\\varepsilon)\\mathbf{h}_i^t + \\sum_{j\\in\\mathcal{N}(i)} \\mathbf{h}_j^t \\right)\n",
    "$$\n",
    "\n",
    "- **GraphSAGE**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W_1 \\mathbf{h}_i^t + W_2 \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j\\in\\mathcal{N}(i)} \\mathbf{h}_j^t\n",
    "$$\n",
    "\n",
    "- **Graph Attention Networks (GAT)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = \\sum_{j\\in\\mathcal{N}(i)\\cup \\{i\\}} \\alpha_{ij} W \\mathbf{h}_j^t,\\\\\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( LeakyReLU(\\mathbf{a}[W\\mathbf{h}_i^t \\| W\\mathbf{h}_j^t])\\right)}{\\sum_{k\\in\\mathcal{N}(i) \\cup \\{i\\}}\\exp\\left( LeakyReLU(\\mathbf{a}[W\\mathbf{h}_i^t \\| W\\mathbf{h}_k^t])\\right)}\n",
    "$$\n",
    "\n",
    "**Exercise 3:** Use the molecular graphs prepared in the previous exercise to predict compound solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f6b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "train_loader = GraphDataLoader(featurizer(split['train']), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = GraphDataLoader(featurizer(split['valid']), batch_size=batch_size)\n",
    "test_loader = GraphDataLoader(featurizer(split['test']), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        ...\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be6c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 10\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # model preparation\n",
    "    model = GraphNeuralNetwork(hidden_size)  # TODO: you can add more hyperparameters if needed\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = ... # TODO: define an optimizer\n",
    "    loss_fn = ...  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            preds = model(x, edge_index, batch)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# training\n",
    "model = train(train_loader, valid_loader)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "\n",
    "print(f'RMSE = {rmse_score:.2f}')\n",
    "assert rmse_score < 1.4, \"It should be possible to obtain RMSE lower than 1.4\"\n",
    "print('Looks OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cdc10",
   "metadata": {},
   "source": [
    "# Explainability of GNN predictions: Grad-CAM\n",
    "\n",
    "In many applications of machine learning, it is crucial to understand model predictions. For example, it is important to understand why our model predicted toxicity if we want to improve the chemical structure and get rid of this liability. In graph neural networks, we can create an importance map overlayed on top of the chemical structure to point at the atoms important for the prediction.\n",
    "\n",
    "Grad-CAM is an explainability method developed for convolutional neural networks (images), and can be adapted to graph convolutions. The representation after the last graph layer is called an **activation map** $F$, where $F_{i,j}$ represents the $i$-th feature of the $j$-th atom. To find out which atomic features correlate with the prediction, we can compute gradients of the predicted class probability w.r.t. the activation map features. Next, these gradients $\\alpha$ are multiplied by the values in the activation maps to calculate per-atom importances.\n",
    "\n",
    "$$\n",
    "\\alpha_k^{l,c}=\\frac{1}{N}\\sum_{n=1}^N \\frac{\\partial y^c}{\\partial F_{k,n}^l},\\\\\n",
    "L^c(l,n) = ReLU\\left(\\sum_k \\alpha_k^{l,c} F_{k,n}^l (X, A)\\right)\n",
    "$$\n",
    "\n",
    "**(optional) Exercise 4.** Copy the graph network code from the previous exercise into correct places, train this modified network, and complete the Grad-CAM code to see the prediction explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ae890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        ...  # TODO: copy from Exercise 3\n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        self.final_conv_grads = grad\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        ...  # TODO: copy from Exercise 3\n",
    "        with torch.enable_grad():\n",
    "            self.final_conv_acts = ...  # TODO: assign convolution activations here\n",
    "        self.final_conv_acts.register_hook(self.activations_hook)\n",
    "        ...  # TODO: copy from Exercise 3\n",
    "        return ...  # TODO: copy from Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train this network using the script in Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "72c19265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def grad_cam(final_conv_acts, final_conv_grads):\n",
    "    node_heat_map = []\n",
    "    alphas = ...  # TODO (formula 1)\n",
    "    for n in range(final_conv_acts.shape[0]): # nth node\n",
    "        node_heat = ...  # TODO (formula 2)\n",
    "        node_heat_map.append(node_heat)\n",
    "    return node_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848a2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = featurizer(split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657945f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from IPython.display import SVG\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "compound_idx = 110\n",
    "mol = Chem.MolFromSmiles(split['test'].iloc[compound_idx].Drug)\n",
    "\n",
    "data = test_set[compound_idx]\n",
    "x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "model(x, edge_index, torch.zeros(x.shape[0], dtype=torch.int64))\n",
    "atom_weights = grad_cam(model.final_conv_acts, model.final_conv_grads)\n",
    "\n",
    "atom_weights = np.array(atom_weights)\n",
    "if (atom_weights > 0.).any():\n",
    "    atom_weights = atom_weights / atom_weights.max() / 2\n",
    "\n",
    "if len(atom_weights) > 0:\n",
    "    norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.get_cmap('bwr')\n",
    "    plt_colors = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    atom_colors = {\n",
    "        i: plt_colors.to_rgba(atom_weights[i]) for i in range(len(atom_weights))\n",
    "    }\n",
    "    highlight_kwargs = {\n",
    "        'highlightAtoms': list(range(len(atom_weights))),\n",
    "        'highlightBonds': [],\n",
    "        'highlightAtomColors': atom_colors\n",
    "    }\n",
    "\n",
    "d = rdMolDraw2D.MolDraw2DSVG(500, 500) # or MolDraw2DCairo to get PNGs\n",
    "rdMolDraw2D.PrepareAndDrawMolecule(d, mol, **highlight_kwargs)\n",
    "d.FinishDrawing()\n",
    "svg = d.GetDrawingText()\n",
    "svg = svg.replace('svg:', '')\n",
    "SVG(svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
